{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad933b50-5ae6-46e5-a52b-3948c7abfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import LGConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_scatter import scatter\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c508ce1-6d7f-43f0-873f-b067e83f9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickstream = pl.read_parquet(\"clickstream.pq\")\n",
    "df_event = pl.read_parquet('events.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555811bd-b955-4580-96ab-eba63fa1bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = df_clickstream['event_date'].max() - timedelta(days=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c531fded-17f3-4f89-9915-804ee2dcc21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickstream = df_clickstream.filter(df_clickstream['event_date'] <= treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5926ac9-af7b-4a1b-9681-0e085630363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickstream = df_clickstream.filter(df_clickstream['event_date'] > treshold - timedelta(days=31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316f56f9-e8f1-4597-89e6-a510c79b9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_df(df_clickstream: pl.LazyFrame, df_event: pl.LazyFrame,\n",
    "                      recency_lambda=30, alpha=0.5) -> pd.DataFrame:\n",
    "    df = (\n",
    "        df_clickstream\n",
    "        .join(df_event, on=\"event\", how=\"inner\")\n",
    "        .group_by([\"node\", \"cookie\"])\n",
    "        .agg([\n",
    "            pl.col(\"event_date\").max().alias(\"event_date\"),\n",
    "            pl.col(\"is_contact\").max().alias(\"is_contact\"),\n",
    "        ])\n",
    "        .with_columns((pl.col(\"is_contact\") + 0.5).alias(\"is_contact\"))\n",
    "        .with_columns(pl.count().over(\"node\").alias(\"node_count\"))\n",
    "        .filter(pl.col(\"node_count\") >= 100)\n",
    "        .drop(\"node_count\")\n",
    "    )\n",
    "    df_pd = df.select([\"cookie\",\"node\",\"event_date\",\"is_contact\"]).to_pandas()\n",
    "    df_pd[\"cookie\"] = df_pd[\"cookie\"].astype(str)\n",
    "    df_pd[\"node\"]   = df_pd[\"node\"].astype(str)\n",
    "\n",
    "    max_date = df_pd[\"event_date\"].max()\n",
    "    df_pd[\"age_days\"]  = (max_date - df_pd[\"event_date\"]).dt.days\n",
    "    df_pd[\"recency_w\"] = np.exp(-df_pd[\"age_days\"] / recency_lambda)\n",
    "    df_pd[\"edge_w\"]    = df_pd[\"is_contact\"] + alpha * df_pd[\"recency_w\"]\n",
    "    df_pd[\"cookie\"]    = \"_\" + df_pd[\"cookie\"]\n",
    "    return df_pd\n",
    "\n",
    "class WeightedLGConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        row, col = edge_index\n",
    "        deg = scatter(edge_weight, row, dim=0, dim_size=x.size(0), reduce='sum')\n",
    "        norm = edge_weight / torch.sqrt(deg[row] * deg[col] + 1e-12)\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, emb_dim=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_nodes, emb_dim)\n",
    "        self.convs = nn.ModuleList([WeightedLGConv() for _ in range(num_layers)])\n",
    "    def forward(self, edge_index, edge_weight):\n",
    "        x = self.emb.weight\n",
    "        all_emb = [x]\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_weight)\n",
    "            all_emb.append(x)\n",
    "        return torch.mean(torch.stack(all_emb), dim=0)\n",
    "\n",
    "def prepare_graph_weighted(df: pd.DataFrame):\n",
    "    users = df['cookie'].unique().tolist()\n",
    "    items = df['node'].unique().tolist()\n",
    "    user_map = {u: i for i, u in enumerate(users)}\n",
    "    item_map = {i: idx + len(users) for idx, i in enumerate(items)}\n",
    "    df['u_idx'] = df['cookie'].map(user_map)\n",
    "    df['i_idx'] = df['node'].map(item_map)\n",
    "\n",
    "    e1 = np.stack([df['u_idx'], df['i_idx']], axis=0)\n",
    "    e2 = np.stack([df['i_idx'], df['u_idx']], axis=0)\n",
    "    edge_index  = torch.tensor(np.concatenate([e1, e2], axis=1), dtype=torch.long)\n",
    "    weights     = np.concatenate([df['edge_w'], df['edge_w']], axis=0)\n",
    "    edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "    data = Data(edge_index=edge_index,\n",
    "                edge_weight=edge_weight,\n",
    "                num_nodes=len(users) + len(items))\n",
    "    return data, user_map, item_map\n",
    "\n",
    "def bpr_loss(u_emb, pos_emb, neg_emb):\n",
    "    pos_score = (u_emb * pos_emb).sum(dim=1, keepdim=True)\n",
    "    neg_score = torch.bmm(neg_emb, u_emb.unsqueeze(-1)).squeeze(-1)\n",
    "    return -torch.log(torch.sigmoid(pos_score - neg_score)).mean()\n",
    "\n",
    "def train_lightgcn(data, df: pd.DataFrame, user_map, item_map,\n",
    "                   emb_dim=128, num_layers=3, lr=1e-2,\n",
    "                   weight_decay=1e-5, epochs=100,\n",
    "                   K_neg=10, patience=5, val_k=100,\n",
    "                   dropout=0.1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LightGCN(data.num_nodes, emb_dim, num_layers).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           mode='max',\n",
    "                                                           factor=0.5,\n",
    "                                                           patience=2,\n",
    "                                                           verbose=True)\n",
    "\n",
    "    user_pos = df.groupby('u_idx')['i_idx'].apply(list).to_dict()\n",
    "    train_pos, val_set = {}, []\n",
    "    for u, items in user_pos.items():\n",
    "        if len(items) > 1:\n",
    "            train_pos[u] = items[:-1]\n",
    "            val_set.append((u, items[-1]))\n",
    "        else:\n",
    "            train_pos[u] = items\n",
    "\n",
    "    best_recall, no_improve = 0.0, 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        users = list(train_pos.keys())\n",
    "        np.random.shuffle(users)\n",
    "        for i in range(0, len(users), 2048):\n",
    "            batch = users[i:i+2048]\n",
    "            pos = [np.random.choice(train_pos[u]) for u in batch]\n",
    "            neg = [np.random.choice(list(set(item_map.values()) - set(train_pos[u])), size=K_neg)\n",
    "                   for u in batch]\n",
    "\n",
    "            u_idx   = torch.tensor(batch, device=device)\n",
    "            pos_idx = torch.tensor(pos, device=device)\n",
    "            neg_idx = torch.tensor(neg, device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            emb_all = model(data.edge_index.to(device), data.edge_weight.to(device))\n",
    "            u_emb   = emb_all[u_idx]\n",
    "            pos_emb = emb_all[pos_idx]\n",
    "            neg_emb = emb_all[neg_idx]\n",
    "\n",
    "            loss = bpr_loss(u_emb, pos_emb, neg_emb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            emb_all   = model(data.edge_index.to(device), data.edge_weight.to(device))\n",
    "            user_emb  = emb_all[:len(user_map)]\n",
    "            item_emb  = emb_all[len(user_map):]\n",
    "            scores    = user_emb @ item_emb.T\n",
    "\n",
    "            # popularity penalty\n",
    "            num_users = len(user_map)\n",
    "            df['i_local'] = df['i_idx'] - num_users\n",
    "            cnts = df['i_local'].value_counts().sort_index()\n",
    "            deg = torch.tensor(cnts.values, device=device)\n",
    "            eta = 0.2\n",
    "            penalty = deg.pow(eta)\n",
    "\n",
    "            adjusted = scores / penalty.unsqueeze(0)\n",
    "            topk = adjusted.topk(val_k, dim=1).indices.cpu().numpy()\n",
    "            hits = sum(1 for u,v in val_set if (v - num_users) in topk[u])\n",
    "            recall = hits / len(val_set)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Loss {total_loss:.4f} | Val Recall@{val_k} {recall:.4f}\")\n",
    "        scheduler.step(recall)\n",
    "        if recall > best_recall:\n",
    "            best_recall, no_improve = recall, 0\n",
    "            torch.save(model.state_dict(), 'best_lgcn.pth')\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load('best_lgcn.pth'))\n",
    "    return model\n",
    "\n",
    "def recommend(model, data, df: pd.DataFrame, user_map, item_map,\n",
    "               top_k=300, eta=0.5) -> pd.DataFrame:\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb_all  = model(data.edge_index.to(device), data.edge_weight.to(device))\n",
    "        user_emb = emb_all[:len(user_map)]\n",
    "        item_emb = emb_all[len(user_map):]\n",
    "        scores   = user_emb @ item_emb.T\n",
    "\n",
    "        # compute item frequencies\n",
    "        num_users = len(user_map)\n",
    "        df['i_local'] = df['i_idx'] - num_users\n",
    "        cnts = df['i_local'].value_counts().sort_index()\n",
    "        deg = torch.tensor(cnts.values, device=device, dtype=torch.float)\n",
    "        penalty = deg.pow(eta)\n",
    "\n",
    "        adjusted = scores / penalty.unsqueeze(0)\n",
    "        topk_idx = adjusted.topk(top_k, dim=1).indices.cpu().numpy()\n",
    "\n",
    "    inv_user = {v:k for k,v in user_map.items()}\n",
    "    inv_item = {v-len(user_map):k for k,v in item_map.items()}\n",
    "    recs = []\n",
    "    for u_idx, items in enumerate(topk_idx):\n",
    "        cookie = inv_user[u_idx]\n",
    "        for rank, i_loc in enumerate(items, 1):\n",
    "            node = inv_item[i_loc]\n",
    "            recs.append({'cookie': cookie, 'node': node, 'rank': rank})\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "\n",
    "def get_embeddings_polars(model, user_map, item_map):\n",
    "    \"\"\"\n",
    "    Returns two Polars DataFrames: users and items embeddings with original IDs and embedding lists.\n",
    "    \"\"\"\n",
    "    emb_all = model.emb.weight.detach().cpu().numpy()\n",
    "    num_users = len(user_map)\n",
    "\n",
    "    user_emb = emb_all[:num_users]\n",
    "    item_emb = emb_all[num_users:]\n",
    "\n",
    "    inv_user = {idx: uid for uid, idx in user_map.items()}\n",
    "    inv_item = {idx - num_users: nid for nid, idx in item_map.items()}\n",
    "\n",
    "    users_list = [ {'id': inv_user[i], 'embedding': user_emb[i].tolist()} for i in range(num_users) ]\n",
    "    items_list = [ {'id': inv_item[i], 'embedding': item_emb[i].tolist()} for i in range(len(item_emb)) ]\n",
    "\n",
    "    user_df = pl.DataFrame(users_list)\n",
    "    item_df = pl.DataFrame(items_list)\n",
    "\n",
    "    return user_df, item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e263b5-f12f-4acf-99f2-caea4c2c75e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_893078/3488182649.py:24: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  .with_columns(pl.count().over(\"node\").alias(\"node_count\"))\n"
     ]
    }
   ],
   "source": [
    "df_pd = build_training_df(df_clickstream, df_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c762a9da-c4ba-47ef-85ea-47ef7f9fa991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cookie</th>\n",
       "      <th>node</th>\n",
       "      <th>event_date</th>\n",
       "      <th>is_contact</th>\n",
       "      <th>age_days</th>\n",
       "      <th>recency_w</th>\n",
       "      <th>edge_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_74314</td>\n",
       "      <td>71520</td>\n",
       "      <td>2025-01-23 13:38:24</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.935507</td>\n",
       "      <td>1.967753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_120524</td>\n",
       "      <td>3015</td>\n",
       "      <td>2025-01-14 21:03:22</td>\n",
       "      <td>0.5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.693041</td>\n",
       "      <td>0.846520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_146748</td>\n",
       "      <td>152476</td>\n",
       "      <td>2025-01-23 23:40:39</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.935507</td>\n",
       "      <td>0.967753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_86072</td>\n",
       "      <td>122304</td>\n",
       "      <td>2025-01-23 18:28:44</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.935507</td>\n",
       "      <td>0.967753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_92049</td>\n",
       "      <td>71520</td>\n",
       "      <td>2025-01-11 12:27:24</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.627089</td>\n",
       "      <td>0.813545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4336577</th>\n",
       "      <td>_59659</td>\n",
       "      <td>152714</td>\n",
       "      <td>2025-01-19 19:31:51</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.818731</td>\n",
       "      <td>0.909365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4336578</th>\n",
       "      <td>_129938</td>\n",
       "      <td>229403</td>\n",
       "      <td>2025-01-12 16:43:08</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.648344</td>\n",
       "      <td>0.824172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4336579</th>\n",
       "      <td>_88987</td>\n",
       "      <td>51163</td>\n",
       "      <td>2025-01-25 11:18:04</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4336580</th>\n",
       "      <td>_89332</td>\n",
       "      <td>320948</td>\n",
       "      <td>2025-01-17 16:57:53</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.765928</td>\n",
       "      <td>0.882964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4336581</th>\n",
       "      <td>_64743</td>\n",
       "      <td>214241</td>\n",
       "      <td>2025-01-12 11:22:56</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.648344</td>\n",
       "      <td>0.824172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4336582 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cookie    node          event_date  is_contact  age_days  recency_w  \\\n",
       "0         _74314   71520 2025-01-23 13:38:24         1.5         2   0.935507   \n",
       "1        _120524    3015 2025-01-14 21:03:22         0.5        11   0.693041   \n",
       "2        _146748  152476 2025-01-23 23:40:39         0.5         2   0.935507   \n",
       "3         _86072  122304 2025-01-23 18:28:44         0.5         2   0.935507   \n",
       "4         _92049   71520 2025-01-11 12:27:24         0.5        14   0.627089   \n",
       "...          ...     ...                 ...         ...       ...        ...   \n",
       "4336577   _59659  152714 2025-01-19 19:31:51         0.5         6   0.818731   \n",
       "4336578  _129938  229403 2025-01-12 16:43:08         0.5        13   0.648344   \n",
       "4336579   _88987   51163 2025-01-25 11:18:04         0.5         0   1.000000   \n",
       "4336580   _89332  320948 2025-01-17 16:57:53         0.5         8   0.765928   \n",
       "4336581   _64743  214241 2025-01-12 11:22:56         0.5        13   0.648344   \n",
       "\n",
       "           edge_w  \n",
       "0        1.967753  \n",
       "1        0.846520  \n",
       "2        0.967753  \n",
       "3        0.967753  \n",
       "4        0.813545  \n",
       "...           ...  \n",
       "4336577  0.909365  \n",
       "4336578  0.824172  \n",
       "4336579  1.000000  \n",
       "4336580  0.882964  \n",
       "4336581  0.824172  \n",
       "\n",
       "[4336582 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a2939e-d35f-47f3-a898-b29a859bbd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, u_map, i_map = prepare_graph_weighted(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33514373-2d13-4d51-a625-c5c4b8cbb657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_893078/3488182649.py:132: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  neg_idx = torch.tensor(neg, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 40.1735 | Val Recall@100 0.0118\n",
      "Epoch 1 | Loss 38.4979 | Val Recall@100 0.0171\n",
      "Epoch 2 | Loss 37.8220 | Val Recall@100 0.0898\n",
      "Epoch 3 | Loss 36.1555 | Val Recall@100 0.2337\n",
      "Epoch 4 | Loss 33.4694 | Val Recall@100 0.2533\n",
      "Epoch 5 | Loss 31.7225 | Val Recall@100 0.2528\n",
      "Epoch 6 | Loss 30.8246 | Val Recall@100 0.2524\n",
      "Epoch 7 | Loss 30.3837 | Val Recall@100 0.2518\n",
      "Epoch 00008: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch 8 | Loss 29.9733 | Val Recall@100 0.2521\n",
      "Epoch 9 | Loss 30.0529 | Val Recall@100 0.2521\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "model = train_lightgcn(data, df_pd, u_map, i_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a05fa67-b3a3-431b-93a8-b826c71c870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = recommend(model, data, df_pd, u_map, i_map, eta=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8618efa7-cb2e-412a-8fa3-3c7409a29992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_emb_df, item_emb_df = get_embeddings_polars(model, u_map, i_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "626074de-af01-4c07-9737-266273338115",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pl.from_pandas(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caec3e54-25d2-4ebc-9c72-b8d70668cb9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (34_809_000, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cookie</th><th>node</th><th>rank</th></tr><tr><td>i64</td><td>u32</td><td>i64</td></tr></thead><tbody><tr><td>74314</td><td>71546</td><td>1</td></tr><tr><td>74314</td><td>71549</td><td>2</td></tr><tr><td>74314</td><td>71511</td><td>3</td></tr><tr><td>74314</td><td>71547</td><td>4</td></tr><tr><td>74314</td><td>71514</td><td>5</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>134812</td><td>299995</td><td>296</td></tr><tr><td>134812</td><td>313801</td><td>297</td></tr><tr><td>134812</td><td>371454</td><td>298</td></tr><tr><td>134812</td><td>163826</td><td>299</td></tr><tr><td>134812</td><td>219557</td><td>300</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (34_809_000, 3)\n",
       "┌────────┬────────┬──────┐\n",
       "│ cookie ┆ node   ┆ rank │\n",
       "│ ---    ┆ ---    ┆ ---  │\n",
       "│ i64    ┆ u32    ┆ i64  │\n",
       "╞════════╪════════╪══════╡\n",
       "│ 74314  ┆ 71546  ┆ 1    │\n",
       "│ 74314  ┆ 71549  ┆ 2    │\n",
       "│ 74314  ┆ 71511  ┆ 3    │\n",
       "│ 74314  ┆ 71547  ┆ 4    │\n",
       "│ 74314  ┆ 71514  ┆ 5    │\n",
       "│ …      ┆ …      ┆ …    │\n",
       "│ 134812 ┆ 299995 ┆ 296  │\n",
       "│ 134812 ┆ 313801 ┆ 297  │\n",
       "│ 134812 ┆ 371454 ┆ 298  │\n",
       "│ 134812 ┆ 163826 ┆ 299  │\n",
       "│ 134812 ┆ 219557 ┆ 300  │\n",
       "└────────┴────────┴──────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    submission_df\n",
    "    .with_columns(\n",
    "        pl.col('cookie')\n",
    "        .str.strip_chars('_')\n",
    "        .cast(pl.Int64)\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col('node')\n",
    "        .cast(pl.UInt32)\n",
    "        .alias('node')\n",
    "    )\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bf72d5f-199c-45ba-89bc-aa5f2cc58f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = (\n",
    "    user_emb_df\n",
    "    .with_columns(\n",
    "        pl.col('id')\n",
    "        .str.strip_chars('_')\n",
    "        .cast(pl.Int64)\n",
    "        .alias('cookie')\n",
    "    )\n",
    "    .drop(pl.col('id'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe5911d5-cb88-4527-af55-1106a5fa958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings = (\n",
    "    item_emb_df\n",
    "    .with_columns(\n",
    "        pl.col('id')\n",
    "        .cast(pl.Int64)\n",
    "        .alias('node')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00597e21-b34f-44a9-b968-67b8c2ffe454",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings.write_parquet('retrieval_data/user_graph_emb_28d.pq')\n",
    "item_embeddings.write_parquet('retrieval_data/item_graph_emb_28d.pq')\n",
    "df.write_parquet('retrieval_data/top300_graph_emb_28d.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7204d41c-2db0-4019-909c-ffe624bd06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickstream = pl.read_parquet(\"clickstream.pq\")\n",
    "\n",
    "df_past = df_clickstream.filter(df_clickstream['event_date']<= treshold)\n",
    "\n",
    "df_eval = df_clickstream.filter(df_clickstream['event_date']> treshold)[['cookie', 'node', 'event']]\n",
    "df_eval = df_eval.join(df_past, on=['cookie', 'node'], how='anti')\n",
    "df_eval = df_eval.filter(\n",
    "    pl.col('event').is_in(\n",
    "        df_event.filter(pl.col('is_contact')==1)['event'].unique()\n",
    "    )\n",
    ")\n",
    "df_eval = df_eval.filter(\n",
    "        pl.col('cookie').is_in(df_past['cookie'].unique())\n",
    "    ).filter(\n",
    "        pl.col('node').is_in(df_past['node'].unique())\n",
    "    )\n",
    "df_eval = df_eval.unique(['cookie', 'node'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0159f1bd-c237-4e97-8d0d-7c70ef1b16d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df_eval.filter(pl.col('cookie').is_in(df.select('cookie').unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1448187b-23c2-4f5e-b7d7-045913e1b6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2662449329037006"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import recall_at\n",
    "\n",
    "recall_at(df_eval, df, k=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc223b52-66b3-43f5-9d5c-94e125ca87ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0531678296018784"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_at(df_eval, df.filter(df['rank'] <=40), k=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ed29b-35a4-4e27-8e54-286847c96fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env310)",
   "language": "python",
   "name": "env310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
