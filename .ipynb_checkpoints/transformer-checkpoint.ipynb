{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2828b494-27db-4df1-828a-0e6789dc7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import timedelta\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "df_clickstream = pl.read_parquet(f'clickstream.pq')\n",
    "df_event = pl.read_parquet(f'events.pq')\n",
    "df_cat = pl.read_parquet(f'cat_features.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958efdb3-241a-4a97-8e30-70e2249bf680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickstream = df_clickstream.join(df_cat.select('item', 'category', 'location'), on='item', how='inner')\n",
    "df_clickstream = df_clickstream.join(df_event, on='event', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac18319a-6580-4ba0-a6ea-49b32e4919eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickstream = df_clickstream.sort('event_date').group_by(['cookie', 'node']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb99888a-00f9-4d97-b64a-1ecc1cbe291c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6_469_093, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cookie</th><th>node</th><th>item</th><th>event</th><th>event_date</th><th>platform</th><th>surface</th><th>category</th><th>location</th><th>is_contact</th></tr><tr><td>i64</td><td>u32</td><td>i64</td><td>i64</td><td>datetime[ns]</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>113587</td><td>313781</td><td>20516204</td><td>17</td><td>2025-01-13 11:30:03</td><td>2</td><td>2</td><td>35</td><td>9508</td><td>0</td></tr><tr><td>145917</td><td>287332</td><td>5816253</td><td>17</td><td>2025-01-15 08:56:52</td><td>3</td><td>11</td><td>35</td><td>2348</td><td>0</td></tr><tr><td>139139</td><td>155650</td><td>6179547</td><td>17</td><td>2025-01-24 17:43:09</td><td>3</td><td>11</td><td>50</td><td>1234</td><td>0</td></tr><tr><td>10630</td><td>264108</td><td>18274016</td><td>17</td><td>2025-01-17 06:35:03</td><td>2</td><td>11</td><td>51</td><td>2903</td><td>0</td></tr><tr><td>82332</td><td>51164</td><td>28268984</td><td>17</td><td>2025-01-10 00:11:51</td><td>2</td><td>2</td><td>49</td><td>1503</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>3507</td><td>71511</td><td>14917618</td><td>17</td><td>2025-01-16 22:28:38</td><td>2</td><td>2</td><td>7</td><td>4577</td><td>0</td></tr><tr><td>7156</td><td>348051</td><td>15880766</td><td>17</td><td>2025-01-16 09:49:44</td><td>2</td><td>8</td><td>51</td><td>2354</td><td>0</td></tr><tr><td>65535</td><td>200418</td><td>12471077</td><td>17</td><td>2025-01-12 22:26:40</td><td>3</td><td>11</td><td>57</td><td>2348</td><td>0</td></tr><tr><td>5564</td><td>229339</td><td>7479809</td><td>17</td><td>2025-01-15 12:40:15</td><td>2</td><td>2</td><td>51</td><td>4351</td><td>0</td></tr><tr><td>44278</td><td>187769</td><td>5053439</td><td>17</td><td>2025-01-17 18:11:32</td><td>3</td><td>2</td><td>53</td><td>2348</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6_469_093, 10)\n",
       "┌────────┬────────┬──────────┬───────┬───┬─────────┬──────────┬──────────┬────────────┐\n",
       "│ cookie ┆ node   ┆ item     ┆ event ┆ … ┆ surface ┆ category ┆ location ┆ is_contact │\n",
       "│ ---    ┆ ---    ┆ ---      ┆ ---   ┆   ┆ ---     ┆ ---      ┆ ---      ┆ ---        │\n",
       "│ i64    ┆ u32    ┆ i64      ┆ i64   ┆   ┆ i64     ┆ i64      ┆ i64      ┆ i64        │\n",
       "╞════════╪════════╪══════════╪═══════╪═══╪═════════╪══════════╪══════════╪════════════╡\n",
       "│ 113587 ┆ 313781 ┆ 20516204 ┆ 17    ┆ … ┆ 2       ┆ 35       ┆ 9508     ┆ 0          │\n",
       "│ 145917 ┆ 287332 ┆ 5816253  ┆ 17    ┆ … ┆ 11      ┆ 35       ┆ 2348     ┆ 0          │\n",
       "│ 139139 ┆ 155650 ┆ 6179547  ┆ 17    ┆ … ┆ 11      ┆ 50       ┆ 1234     ┆ 0          │\n",
       "│ 10630  ┆ 264108 ┆ 18274016 ┆ 17    ┆ … ┆ 11      ┆ 51       ┆ 2903     ┆ 0          │\n",
       "│ 82332  ┆ 51164  ┆ 28268984 ┆ 17    ┆ … ┆ 2       ┆ 49       ┆ 1503     ┆ 0          │\n",
       "│ …      ┆ …      ┆ …        ┆ …     ┆ … ┆ …       ┆ …        ┆ …        ┆ …          │\n",
       "│ 3507   ┆ 71511  ┆ 14917618 ┆ 17    ┆ … ┆ 2       ┆ 7        ┆ 4577     ┆ 0          │\n",
       "│ 7156   ┆ 348051 ┆ 15880766 ┆ 17    ┆ … ┆ 8       ┆ 51       ┆ 2354     ┆ 0          │\n",
       "│ 65535  ┆ 200418 ┆ 12471077 ┆ 17    ┆ … ┆ 11      ┆ 57       ┆ 2348     ┆ 0          │\n",
       "│ 5564   ┆ 229339 ┆ 7479809  ┆ 17    ┆ … ┆ 2       ┆ 51       ┆ 4351     ┆ 0          │\n",
       "│ 44278  ┆ 187769 ┆ 5053439  ┆ 17    ┆ … ┆ 2       ┆ 53       ┆ 2348     ┆ 0          │\n",
       "└────────┴────────┴──────────┴───────┴───┴─────────┴──────────┴──────────┴────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut = df_clickstream[\"event_date\"].max() - timedelta(days=28)\n",
    "df_clickstream = df_clickstream.filter(pl.col(\"event_date\") <= cut)\n",
    "df_clickstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6cecc30-33a4-4502-9055-82c70eca2d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 20\n",
    "embed_dim = 96\n",
    "n_heads = 3 \n",
    "n_layers = 2\n",
    "batch_size = 512\n",
    "n_epochs = 5\n",
    "top_k = 300\n",
    "\n",
    "def prepare_data(df):\n",
    "    # create sessions based on 30min inactivity\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"event_date\").cast(pl.Int64) // 1e9\n",
    "    ).sort([\"cookie\", \"event_date\"])\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.col(\"event_date\").diff().over(\"cookie\").alias(\"time_diff\")\n",
    "    ).with_columns(\n",
    "        pl.when(pl.col(\"time_diff\") > 1800).then(1).otherwise(0).alias(\"session_start\")\n",
    "    ).with_columns(\n",
    "        pl.col(\"session_start\").cum_sum().over(\"cookie\").alias(\"session_id\")\n",
    "    )\n",
    "    \n",
    "    contact_df = df.filter(pl.col(\"is_contact\")==1)\n",
    "    \n",
    "    sequences = (\n",
    "        df.join(\n",
    "            contact_df.select([\"cookie\", pl.col(\"session_id\").alias(\"target_session\")]),\n",
    "            on=\"cookie\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .filter(pl.col(\"session_id\") < pl.col(\"target_session\"))\n",
    "    )\n",
    "    \n",
    "    return sequences.group_by([\"cookie\", \"target_session\"]).agg(\n",
    "        pl.col(\"node\").tail(max_seq_length).alias(\"node_seq\"),\n",
    "        pl.col(\"event\").tail(max_seq_length).alias(\"event_seq\"),\n",
    "        pl.col(\"category\").tail(max_seq_length).alias(\"category_seq\"),\n",
    "        pl.col(\"platform\").last().alias(\"platform\"),\n",
    "        pl.col(\"surface\").last().alias(\"surface\"),\n",
    "        pl.col(\"location\").last().alias(\"location\"),\n",
    "        pl.col(\"node\").last().alias(\"target_node\")\n",
    "    )\n",
    "\n",
    "def create_vocab_mappings(df):\n",
    "    node_list = df[\"node\"].unique().to_list()\n",
    "    node_vocab = {v: i+1 for i, v in enumerate(node_list)}\n",
    "    node_reverse = {i+1: v for i, v in enumerate(node_list)}\n",
    "    node_reverse[0] = \"PAD\"\n",
    "    \n",
    "    return {\n",
    "        \"node\": defaultdict(lambda: 0, node_vocab),\n",
    "        \"node_reverse\": node_reverse,\n",
    "        \"event\": defaultdict(lambda: 0, {v: i+1 for i, v in enumerate(df[\"event\"].unique().to_list())}),\n",
    "        \"category\": defaultdict(lambda: 0, {v: i+1 for i, v in enumerate(df[\"category\"].unique().to_list())}),\n",
    "        \"platform\": defaultdict(lambda: 0, {v: i+1 for i, v in enumerate(df[\"platform\"].unique().to_list())}),\n",
    "        \"surface\": defaultdict(lambda: 0, {v: i+1 for i, v in enumerate(df[\"surface\"].unique().to_list())}),\n",
    "        \"location\": defaultdict(lambda: 0, {v: i+1 for i, v in enumerate(df[\"location\"].unique().to_list())}),\n",
    "    }\n",
    "\n",
    "class ClickDataset(Dataset):\n",
    "    def __init__(self, data, vocabs, is_training):\n",
    "        self.data = data\n",
    "        self.vocabs = vocabs\n",
    "        self.is_training = is_training\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.height\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.row(idx, named=True)\n",
    "        item = {\n",
    "            \"nodes\": torch.LongTensor(self.pad_sequence([self.vocabs[\"node\"][v] for v in row[\"node_seq\"]])),\n",
    "            \"events\": torch.LongTensor(self.pad_sequence([self.vocabs[\"event\"][v] for v in row[\"event_seq\"]])),\n",
    "            \"categories\": torch.LongTensor(self.pad_sequence([self.vocabs[\"category\"][v] for v in row[\"category_seq\"]])),\n",
    "            \"platform\": torch.LongTensor([self.vocabs[\"platform\"][row[\"platform\"]]]),\n",
    "            \"surface\": torch.LongTensor([self.vocabs[\"surface\"][row[\"surface\"]]]),\n",
    "            \"location\": torch.LongTensor([self.vocabs[\"location\"][row[\"location\"]]]),\n",
    "            # \"target\": torch.tensor(self.vocabs[\"node\"][row[\"target_node\"]], dtype=torch.long),\n",
    "            \"cookie\": row[\"cookie\"]  # Keep cookie for final predictions\n",
    "        }\n",
    "        if self.is_training:\n",
    "            item[\"target\"] = torch.tensor(self.vocabs[\"node\"][row[\"target_node\"]], dtype=torch.long)\n",
    "        else:\n",
    "            item[\"cookie\"] = row[\"cookie\"]\n",
    "        return item\n",
    "    \n",
    "    def pad_sequence(self, seq):\n",
    "        return seq[-max_seq_length:] + [0]*(max_seq_length - len(seq))\n",
    "\n",
    "class ContactPredictor(nn.Module):\n",
    "    def __init__(self, vocabs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.node_emb = nn.Embedding(len(vocabs[\"node\"])+1, embed_dim, padding_idx=0)\n",
    "        self.event_emb = nn.Embedding(len(vocabs[\"event\"])+1, embed_dim//4, padding_idx=0)\n",
    "        self.cat_emb = nn.Embedding(len(vocabs[\"category\"])+1, embed_dim//4, padding_idx=0)\n",
    "        \n",
    "        # transformer\n",
    "        self.pos_embedding = nn.Embedding(max_seq_length, embed_dim + embed_dim//4*2)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim + embed_dim//4*2,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=embed_dim*2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.context_emb = nn.ModuleDict({\n",
    "            'platform': nn.Embedding(len(vocabs[\"platform\"])+1, embed_dim//4),\n",
    "            'surface': nn.Embedding(len(vocabs[\"surface\"])+1, embed_dim//4),\n",
    "            'location': nn.Embedding(len(vocabs[\"location\"])+1, embed_dim//4)\n",
    "        })\n",
    "        \n",
    "        seq_dim     = embed_dim + 2*(embed_dim//4)\n",
    "        context_dim = 3*(embed_dim//4)\n",
    "        self.context_proj = nn.Sequential(\n",
    "            nn.Linear(seq_dim + context_dim, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(embed_dim, len(vocabs[\"node\"])+1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        nodes = x[\"nodes\"].to(self.node_emb.weight.device)\n",
    "        events = x[\"events\"].to(self.event_emb.weight.device)\n",
    "        categories = x[\"categories\"].to(self.cat_emb.weight.device)\n",
    "        \n",
    "        node_emb = self.node_emb(nodes)\n",
    "        event_emb = self.event_emb(events)\n",
    "        cat_emb = self.cat_emb(categories)\n",
    "        \n",
    "        seq_emb = torch.cat([node_emb, event_emb, cat_emb], dim=-1)\n",
    "        \n",
    "        positions = self.pos_embedding(torch.arange(max_seq_length, device=nodes.device))\n",
    "        seq_emb += positions.unsqueeze(0)\n",
    "        \n",
    "        transformer_out = self.transformer(seq_emb)\n",
    "        seq_encoded = transformer_out.mean(dim=1) \n",
    "        \n",
    "        platform_emb = self.context_emb['platform'](x[\"platform\"].squeeze().to(nodes.device))\n",
    "        surface_emb = self.context_emb['surface'](x[\"surface\"].squeeze().to(nodes.device))\n",
    "        location_emb = self.context_emb['location'](x[\"location\"].squeeze().to(nodes.device))\n",
    "        \n",
    "        combined = torch.cat([\n",
    "            seq_encoded,\n",
    "            platform_emb,\n",
    "            surface_emb,\n",
    "            location_emb\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return self.fc(self.context_proj(combined))\n",
    "\n",
    "def generate_efficient_predictions(df, model, vocabs):\n",
    "    latest_seq = df.sort([\"cookie\", \"event_date\"]).group_by(\"cookie\").agg(\n",
    "        pl.col(\"node\").tail(20).alias(\"node_seq\"),\n",
    "        pl.col(\"event\").tail(20).alias(\"event_seq\"),\n",
    "        pl.col(\"category\").tail(20).alias(\"category_seq\"),\n",
    "        pl.col(\"platform\").last(),\n",
    "        pl.col(\"surface\").last(),\n",
    "        pl.col(\"location\").last()\n",
    "    )\n",
    "\n",
    "    user_histories = df.group_by(\"cookie\").agg(pl.col(\"node\").unique().alias(\"hist_nodes\"))\n",
    "\n",
    "    pred_ds = ClickDataset(latest_seq, vocabs, is_training=False)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in DataLoader(pred_ds, batch_size=batch_size):\n",
    "            outputs = model(batch)\n",
    "            probs = torch.softmax(outputs, dim=-1)\n",
    "            top_probs, top_idxs = torch.topk(probs, k=300)\n",
    "\n",
    "            for cookie, idxs, p in zip(batch[\"cookie\"], top_idxs.cpu().numpy(), top_probs.cpu().numpy()):\n",
    "                recommended = [vocabs[\"node_reverse\"].get(idx, \"PAD\") for idx in idxs if idx in vocabs[\"node_reverse\"]]\n",
    "                user_history = set(user_histories.filter(pl.col(\"cookie\") == cookie)[\"hist_nodes\"].to_list()[0])\n",
    "\n",
    "                new_recs = [(rank+1, node, prob) for rank, (node, prob) in enumerate(zip(recommended, p)) if node not in user_history][:300]\n",
    "\n",
    "                for rank, node, prob in new_recs:\n",
    "                    all_preds.append({\n",
    "                        \"cookie\": cookie,\n",
    "                        \"node\": node,\n",
    "                        \"rank\": rank,\n",
    "                        \"proba\": prob\n",
    "                    })\n",
    "\n",
    "    return pl.DataFrame(all_preds)\n",
    "\n",
    "def train_model(df):\n",
    "    seq_df = prepare_data(df)\n",
    "    vocabs = create_vocab_mappings(df)\n",
    "    \n",
    "    train_df = seq_df.filter(pl.col(\"target_session\") % 10 < 18)\n",
    "    val_df = seq_df.filter(pl.col(\"target_session\") % 10 >= 8)\n",
    "    \n",
    "    train_ds = ClickDataset(train_df, vocabs, is_training=True)\n",
    "    val_ds = ClickDataset(val_df, vocabs, is_training=True)\n",
    "    \n",
    "    model = ContactPredictor(vocabs)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(DataLoader(train_ds, batch_size=batch_size, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, batch[\"target\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in DataLoader(val_ds, batch_size=batch_size):\n",
    "                outputs = model(batch)\n",
    "                val_loss += criterion(outputs, batch[\"target\"]).item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss {train_loss/len(train_ds):.4f}, Val Loss {val_loss/len(val_ds):.4f}\")\n",
    "    \n",
    "    return model, vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f5e9342-9946-4a72-86f5-238536bfe2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:07<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 0.0252, Val Loss 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:07<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 0.0202, Val Loss 0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 0.0169, Val Loss 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:07<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 0.0162, Val Loss 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:07<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 0.0158, Val Loss 0.0157\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model, vocabs = train_model(df_clickstream)\n",
    "    \n",
    "    predictions = generate_efficient_predictions(df_clickstream, model, vocabs)\n",
    "    \n",
    "    predictions.write_parquet(\"retrieval_data/transformer2_28d.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8bad1c-6b6b-41a8-be27-2fc7bb517385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e9f6f63-fdb7-436e-872d-308178cb8446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clickstream2= pl.read_parquet(f'clickstream.pq')\n",
    "\n",
    "df_eval = df_clickstream2.filter(df_clickstream2['event_date']> cut)[['cookie', 'node', 'event']]\n",
    "df_eval = df_eval.join(df_clickstream, on=['cookie', 'node'], how='anti')\n",
    "df_eval = df_eval.filter(\n",
    "    pl.col('event').is_in(\n",
    "        df_event.filter(pl.col('is_contact')==1)['event'].unique()\n",
    "    )\n",
    ")\n",
    "\n",
    "df_eval = df_eval.unique(['cookie', 'node'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a45645-1a6e-4711-837d-de79f1d407f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2476348211618965"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import create_features, recall_at, fit_lgb_ranker\n",
    "recall_at(df_eval, predictions, k=300) #0.1273 0.2258 0.22945 0.18153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01209780-c09e-41b7-95e2-8a1bb94d45ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env310)",
   "language": "python",
   "name": "env310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
